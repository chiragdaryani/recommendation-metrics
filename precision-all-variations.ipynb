{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AP@K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals = ['p_a', 'p_b']\n",
    "predicted = ['p_d', 'p_a', 'p_c', 'p_b', 'p_e', 'p_f']\n",
    "\n",
    "#If we compare the actuals vs. the predicted, then we can see that p_b shows up on the 2nd rank and p_d on the 4th one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apk(y_true, y_pred, k_max=0):\n",
    "  # Check if all elements in lists are unique\n",
    "  if len(set(y_true)) != len(y_true):\n",
    "    raise ValueError(\"Values in y_true are not unique\")\n",
    "  if len(set(y_pred)) != len(y_pred):\n",
    "    raise ValueError(\"Values in y_pred are not unique\")\n",
    "  if k_max != 0:\n",
    "    y_pred = y_pred[:k_max]\n",
    " \n",
    "  correct_predictions = 0\n",
    "  running_sum = 0\n",
    "  for i, yp_item in enumerate(y_pred):\n",
    "    \n",
    "    k = i+1 # our rank starts at 1\n",
    "    \n",
    "    if yp_item in y_true:\n",
    "      correct_predictions += 1\n",
    "      running_sum += correct_predictions/k\n",
    "  return running_sum/len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apk(actuals, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAP@K\n",
    "\n",
    "\n",
    "It averages the AP@K metric for recommendations shown to \"M\" users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals = [\n",
    "    ['p_a', 'p_b'],\n",
    "    ['p_a', 'p_b'],\n",
    "    ['p_a', 'p_b']\n",
    "]\n",
    "\n",
    "predicted = [\n",
    "    ['p_a', 'p_b', 'p_c', 'p_d', 'p_e', 'p_f'], #user 1\n",
    "    ['p_c', 'p_d', 'p_e', 'p_f', 'p_a', 'p_b'], #user 2\n",
    "    ['p_d', 'p_a', 'p_c', 'p_b', 'p_e', 'p_f'], #user 3\n",
    "]\n",
    "\n",
    "# Our actual and predicted lists contains now several lists (one for each user). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "def mapk(actuals, predicted, k=0):\n",
    "  return np.mean([apk(a,p,k) for a,p in product(actuals, predicted)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5888888888888889"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapk(actuals, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another example:\n",
    "\n",
    "actuals = [\n",
    "    ['p_a', 'p_b','p_c', 'p_d', 'p_e', 'p_f'],\n",
    "    ['p_c', 'p_d', 'p_e', 'p_f', 'p_a', 'p_b'],\n",
    "    ['p_d', 'p_a', 'p_c', 'p_b', 'p_e', 'p_f']\n",
    "]\n",
    "\n",
    "predicted = [\n",
    "    ['p_a', 'p_b', 'p_c', 'p_d', 'p_e', 'p_f'], #user 1\n",
    "    ['p_c', 'p_d', 'p_e', 'p_f', 'p_a', 'p_b'], #user 2\n",
    "    ['p_d', 'p_a', 'p_c', 'p_b', 'p_e', 'p_f'], #user 3\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapk(actuals, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Ref:\n",
    "#https://towardsdatascience.com/mean-average-precision-at-k-map-k-clearly-explained-538d8e032d2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "04c8fc554d422d6a1d14c04c415a1ade8391b1a71b74f13f4a73c8d1d438c824"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
